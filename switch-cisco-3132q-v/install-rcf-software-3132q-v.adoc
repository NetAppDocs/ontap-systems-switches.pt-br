---
permalink: switch-cisco-3132q-v/install-rcf-software-3132q-v.html 
sidebar: sidebar 
keywords: ssh, requirement, cluster, switch, health, monitor, cshm, log, collection, feature, cisco 3132q-v 
summary: 'O SSH é um requisito ao usar o Cluster Switch Health Monitor (CSHM) e os recursos de coleta de logs. Para habilitar o SSH em switches de cluster Cisco 3132q-v, primeiro gere as chaves SSH e depois habilite o SSH.' 
---
= Instalar o ficheiro de configuração de referência (RCF)
:allow-uri-read: 
:icons: font
:imagesdir: ../media/


[role="lead"]
Você instala o arquivo de configuração de referência (RCF) após configurar os switches Nexus 3132Q-V pela primeira vez.

.Antes de começar
Verifique as seguintes instalações e conexões:

* Um backup atual da configuração do switch.
* Um cluster totalmente funcional (sem erros nos logs ou problemas semelhantes).
* O RCF atual.
* Uma ligação de consola ao interrutor, necessária ao instalar o RCF.


.Sobre esta tarefa
O procedimento requer o uso de comandos ONTAP e comandos Cisco Nexus 3000 Series switches; os comandos ONTAP são usados, a menos que indicado de outra forma.

Nenhum link operacional entre switches (ISL) é necessário durante este procedimento.  Isso ocorre porque as alterações na versão do RCF podem afetar a conectividade ISL temporariamente.  Para habilitar operações de cluster sem interrupções, o procedimento a seguir migra todos os LIFs do cluster para o switch parceiro operacional enquanto executa as etapas no switch de destino.



== Passo 1: Instale o RCF nos interrutores

. Exiba as portas do cluster em cada nó conetado aos switches do cluster:
+
`network device-discovery show`

+
.Mostrar exemplo
[%collapsible]
====
[listing, subs="+quotes"]
----
cluster1::*> *network device-discovery show*
Node/       Local  Discovered
Protocol    Port   Device (LLDP: ChassisID)  Interface         Platform
----------- ------ ------------------------- ----------------  ------------
cluster1-01/cdp
            e0a    cs1                       Ethernet1/7       N3K-C3132Q-V
            e0d    cs2                       Ethernet1/7       N3K-C3132Q-V
cluster1-02/cdp
            e0a    cs1                       Ethernet1/8       N3K-C3132Q-V
            e0d    cs2                       Ethernet1/8       N3K-C3132Q-V
cluster1-03/cdp
            e0a    cs1                       Ethernet1/1/1     N3K-C3132Q-V
            e0b    cs2                       Ethernet1/1/1     N3K-C3132Q-V
cluster1-04/cdp
            e0a    cs1                       Ethernet1/1/2     N3K-C3132Q-V
            e0b    cs2                       Ethernet1/1/2     N3K-C3132Q-V
cluster1::*>
----
====
. Verifique o status administrativo e operacional de cada porta de cluster.
+
.. Verifique se todas as portas do cluster estão ativas com um status de integridade:
+
`network port show -ipspace Cluster`

+
.Mostrar exemplo
[%collapsible]
====
[listing, subs="+quotes"]
----
cluster1::*> *network port show -ipspace Cluster*
Node: cluster1-01
                                                                       Ignore
                                                  Speed(Mbps) Health   Health
Port      IPspace      Broadcast Domain Link MTU  Admin/Oper  Status   Status
--------- ------------ ---------------- ---- ---- ----------- -------- ------
e0a       Cluster      Cluster          up   9000  auto/100000 healthy false
e0d       Cluster      Cluster          up   9000  auto/100000 healthy false
Node: cluster1-02
                                                                       Ignore
                                                  Speed(Mbps) Health   Health
Port      IPspace      Broadcast Domain Link MTU  Admin/Oper  Status   Status
--------- ------------ ---------------- ---- ---- ----------- -------- ------
e0a       Cluster      Cluster          up   9000  auto/100000 healthy false
e0d       Cluster      Cluster          up   9000  auto/100000 healthy false
8 entries were displayed.
Node: cluster1-03
   Ignore
                                                  Speed(Mbps) Health   Health
Port      IPspace      Broadcast Domain Link MTU  Admin/Oper  Status   Status
--------- ------------ ---------------- ---- ---- ----------- -------- ------
e0a       Cluster      Cluster          up   9000  auto/10000 healthy  false
e0b       Cluster      Cluster          up   9000  auto/10000 healthy  false
Node: cluster1-04
                                                                       Ignore
                                                  Speed(Mbps) Health   Health
Port      IPspace      Broadcast Domain Link MTU  Admin/Oper  Status   Status
--------- ------------ ---------------- ---- ---- ----------- -------- ------
e0a       Cluster      Cluster          up   9000  auto/10000 healthy  false
e0b       Cluster      Cluster          up   9000  auto/10000 healthy  false
cluster1::*>
----
====
.. Verifique se todas as interfaces de cluster (LIFs) estão na porta inicial:
+
`network interface show -vserver Cluster`

+
.Mostrar exemplo
[%collapsible]
====
[listing, subs="+quotes"]
----
cluster1::*> *network interface show -vserver Cluster*
            Logical            Status     Network           Current      Current Is
Vserver     Interface          Admin/Oper Address/Mask      Node         Port    Home
----------- ------------------ ---------- ----------------- ------------ ------- ----
Cluster
            cluster1-01_clus1  up/up     169.254.3.4/23     cluster1-01  e0a     true
            cluster1-01_clus2  up/up     169.254.3.5/23     cluster1-01  e0d     true
            cluster1-02_clus1  up/up     169.254.3.8/23     cluster1-02  e0a     true
            cluster1-02_clus2  up/up     169.254.3.9/23     cluster1-02  e0d     true
            cluster1-03_clus1  up/up     169.254.1.3/23     cluster1-03  e0a     true
            cluster1-03_clus2  up/up     169.254.1.1/23     cluster1-03  e0b     true
            cluster1-04_clus1  up/up     169.254.1.6/23     cluster1-04  e0a     true
            cluster1-04_clus2  up/up     169.254.1.7/23     cluster1-04  e0b     true
cluster1::*>
----
====
.. Verifique se o cluster exibe informações para ambos os switches do cluster:
+
`system cluster-switch show -is-monitoring-enabled-operational true`

+
.Mostrar exemplo
[%collapsible]
====
[listing, subs="+quotes"]
----
cluster1::*> *system cluster-switch show -is-monitoring-enabled-operational true*
Switch                      Type               Address          Model
--------------------------- ------------------ ---------------- ---------------
cs1                         cluster-network    10.0.0.1         NX3132QV
     Serial Number: FOXXXXXXXGS
      Is Monitored: true
            Reason: None
  Software Version: Cisco Nexus Operating System (NX-OS) Software, Version
                    9.3(4)
    Version Source: CDP
cs2                         cluster-network    10.0.0.2         NX3132QV
     Serial Number: FOXXXXXXXGD
      Is Monitored: true
            Reason: None
  Software Version: Cisco Nexus Operating System (NX-OS) Software, Version
                    9.3(4)
    Version Source: CDP
2 entries were displayed.
----
====


+

NOTE: Para o ONTAP 9.8 e posterior, use o comando `system switch ethernet show -is-monitoring-enabled-operational true`.

. Desative a reversão automática nos LIFs do cluster.
+
[listing, subs="+quotes"]
----
cluster1::*> *network interface modify -vserver Cluster -lif * -auto-revert false*
----
+
Certifique-se de que a reversão automática esteja desativada após executar este comando.

. No switch de cluster CS2, encerre as portas conetadas às portas de cluster dos nós.
+
[listing, subs="+quotes"]
----
cs2> *enable*
cs2# *configure*
cs2(config)# *interface eth1/1/1-2,eth1/7-8*
cs2(config-if-range)# *shutdown*
cs2(config-if-range)# *exit*
cs2# *exit*
----
+

NOTE: O número de portas exibidas varia de acordo com o número de nós no cluster.

. Verifique se as portas do cluster falharam nas portas hospedadas no switch de cluster cs1. Isso pode levar alguns segundos.
+
`network interface show -vserver Cluster`

+
.Mostrar exemplo
[%collapsible]
====
[listing, subs="+quotes"]
----
cluster1::*> *network interface show -vserver Cluster*
            Logical           Status     Network            Current       Current Is
Vserver     Interface         Admin/Oper Address/Mask       Node          Port    Home
----------- ----------------- ---------- ------------------ ------------- ------- ----
Cluster
            cluster1-01_clus1 up/up      169.254.3.4/23     cluster1-01   e0a     true
            cluster1-01_clus2 up/up      169.254.3.5/23     cluster1-01   e0a     false
            cluster1-02_clus1 up/up      169.254.3.8/23     cluster1-02   e0a     true
            cluster1-02_clus2 up/up      169.254.3.9/23     cluster1-02   e0a     false
            cluster1-03_clus1 up/up      169.254.1.3/23     cluster1-03   e0a     true
            cluster1-03_clus2 up/up      169.254.1.1/23     cluster1-03   e0a     false
            cluster1-04_clus1 up/up      169.254.1.6/23     cluster1-04   e0a     true
            cluster1-04_clus2 up/up      169.254.1.7/23     cluster1-04   e0a     false
cluster1::*>
----
====
. Verifique se o cluster está em bom estado:
+
`cluster show`

+
.Mostrar exemplo
[%collapsible]
====
[listing, subs="+quotes"]
----
cluster1::*> *cluster show*
Node                 Health  Eligibility   Epsilon
-------------------- ------- ------------  -------
cluster1-01          true    true          false
cluster1-02          true    true          false
cluster1-03          true    true          true
cluster1-04          true    true          false
cluster1::*>
----
====
. Se você ainda não fez isso, salve uma cópia da configuração atual do switch copiando a saída do seguinte comando para um arquivo de texto:
+
`show running-config`

. Registre quaisquer adições personalizadas entre a configuração atual e o arquivo RCF em uso.
+
[NOTE]
====
Certifique-se de configurar o seguinte: * Nome de usuário e senha * Endereço IP de gerenciamento * Gateway padrão * Nome do switch

====
. Salve os detalhes básicos da configuração em `write_erase.cfg` arquivo no bootflash.
+

NOTE: Ao atualizar ou aplicar um novo RCF, você deve apagar as configurações do switch e executar a configuração básica.  Você deve estar conectado à porta serial do console do switch para configurá-lo novamente.

+
`cs2# show run | section "switchname" > bootflash:write_erase.cfg`

+
`cs2# show run | section "hostname" >> bootflash:write_erase.cfg`

+
`cs2# show run | i "username admin password" >> bootflash:write_erase.cfg`

+
`cs2# show run | section "vrf context management" >> bootflash:write_erase.cfg`

+
`cs2# show run | section "interface mgmt0" >> bootflash:write_erase.cfg`

. Para RCF versão 1.12 e posteriores, execute os seguintes comandos:
+
`cs2# echo "hardware access-list tcam region vpc-convergence 256" >> bootflash:write_erase.cfg`

+
`cs2# echo "hardware access-list tcam region racl 256" >> bootflash:write_erase.cfg`

+
`cs2# echo "hardware access-list tcam region e-racl 256" >> bootflash:write_erase.cfg`

+
`cs2# echo "hardware access-list tcam region qos 256" >> bootflash:write_erase.cfg`

+
Veja o artigo da Base de Conhecimento https://kb.netapp.com/on-prem/Switches/Cisco-KBs/How_to_clear_configuration_on_a_Cisco_interconnect_switch_while_retaining_remote_connectivity["Como limpar a configuração em um switch de interconexão Cisco, mantendo a conetividade remota"^] para mais detalhes.

. Verifique se o `write_erase.cfg` o arquivo é preenchido conforme o esperado:
+
`*show file bootflash:write_erase.cfg*`

. Emitir o `write erase` comando para apagar a configuração salva atual:
+
`cs2# *write erase*`

+
`Warning: This command will erase the startup-configuration.`

+
`Do you wish to proceed anyway? (y/n)  [n] *y*`

. Copie a configuração básica salva anteriormente na configuração de inicialização.
+
`cs2# *copy bootflash:write_erase.cfg startup-config*`

. Reinicie o switch:
+
`cs2# *reload*`

+
`This command will reboot the system. (y/n)?  [n] *y*`

. Repita os passos 7 a 14 no interrutor CS1.
. Conecte as portas do cluster de todos os nós do cluster ONTAP aos switches CS1 e CS2.




== Etapa 2: Verifique as conexões do interrutor

. Verifique se as portas do switch conetadas às portas do cluster estão *up*.
+
`show interface brief | grep up`

+
.Mostrar exemplo
[%collapsible]
====
[listing, subs="+quotes"]
----
cs1# *show interface brief | grep up*
.
.
Eth1/1/1      1       eth  access up      none                    10G(D) --
Eth1/1/2      1       eth  access up      none                    10G(D) --
Eth1/7        1       eth  trunk  up      none                   100G(D) --
Eth1/8        1       eth  trunk  up      none                   100G(D) --
.
.
----
====
. Verifique se o ISL entre CS1 e CS2 está funcional:
+
`show port-channel summary`

+
.Mostrar exemplo
[%collapsible]
====
[listing, subs="+quotes"]
----
cs1# *show port-channel summary*
Flags:  D - Down        P - Up in port-channel (members)
        I - Individual  H - Hot-standby (LACP only)
        s - Suspended   r - Module-removed
        b - BFD Session Wait
        S - Switched    R - Routed
        U - Up (port-channel)
        p - Up in delay-lacp mode (member)
        M - Not in use. Min-links not met
--------------------------------------------------------------------------------
Group Port-       Type     Protocol  Member Ports
      Channel
--------------------------------------------------------------------------------
1     Po1(SU)     Eth      LACP      Eth1/31(P)   Eth1/32(P)
cs1#
----
====
. Verifique se os LIFs do cluster reverteram para sua porta inicial:
+
`network interface show -vserver Cluster`

+
.Mostrar exemplo
[%collapsible]
====
[listing, subs="+quotes"]
----
cluster1::*> *network interface show -vserver Cluster*
            Logical            Status     Network            Current             Current Is
Vserver     Interface          Admin/Oper Address/Mask       Node                Port    Home
----------- ------------------ ---------- ------------------ ------------------- ------- ----
Cluster
            cluster1-01_clus1  up/up      169.254.3.4/23     cluster1-01         e0d     true
            cluster1-01_clus2  up/up      169.254.3.5/23     cluster1-01         e0d     true
            cluster1-02_clus1  up/up      169.254.3.8/23     cluster1-02         e0d     true
            cluster1-02_clus2  up/up      169.254.3.9/23     cluster1-02         e0d     true
            cluster1-03_clus1  up/up      169.254.1.3/23     cluster1-03         e0b     true
            cluster1-03_clus2  up/up      169.254.1.1/23     cluster1-03         e0b     true
            cluster1-04_clus1  up/up      169.254.1.6/23     cluster1-04         e0b     true
            cluster1-04_clus2  up/up      169.254.1.7/23     cluster1-04         e0b     true
cluster1::*>
----
====
. Verifique se o cluster está em bom estado:
+
`cluster show`

+
.Mostrar exemplo
[%collapsible]
====
[listing, subs="+quotes"]
----
cluster1::*> *cluster show*
Node                 Health  Eligibility   Epsilon
-------------------- ------- ------------- -------
cluster1-01          true    true          false
cluster1-02          true    true          false
cluster1-03          true    true          true
cluster1-04          true    true          false
cluster1::*>
----
====




== Etapa 3: configurar seu cluster ONTAP

A NetApp recomenda que você use o Gerenciador de sistemas para configurar novos clusters.

O System Manager fornece um fluxo de trabalho simples e fácil para configuração e instalação de cluster, incluindo atribuição de um endereço IP de gerenciamento de nó, inicialização do cluster, criação de uma camada local, configuração de protocolos e provisionamento de armazenamento inicial.

Consultelink:https://docs.netapp.com/us-en/ontap/task_configure_ontap.html["Configure o ONTAP em um novo cluster com o Gerenciador do sistema"] para obter instruções de configuração.

.O que se segue?
Depois de instalar o RCF, vocêlink:configure-ssh-keys.html["verificar a configuração SSH"] .
